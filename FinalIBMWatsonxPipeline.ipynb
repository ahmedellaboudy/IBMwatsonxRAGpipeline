{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWMYeAtG7/ijj7wzxxt4XF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedellaboudy/IBMwatsonxRAGpipeline/blob/main/FinalIBMWatsonxPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXI7QFY3M_w_"
      },
      "outputs": [],
      "source": [
        "# CELL 1: Installations\n",
        "!pip install -q chromadb sentence-transformers transformers torch langchain langchain-community langchain-huggingface accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Import Libraries\n",
        "import json\n",
        "import torch\n",
        "import warnings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from google.colab import files\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "qFGcwvV8PTT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: RAG Pipeline Class\n",
        "class WatsonXRAGPipeline:\n",
        "    def __init__(self, json_file_path: str, persist_directory: str = \"./watsonx_chroma_db\"):\n",
        "        self.json_file_path = json_file_path\n",
        "        self.persist_directory = persist_directory\n",
        "        self.embedding_model = None\n",
        "        self.vectorstore = None\n",
        "        self.llm_chain = None\n",
        "\n",
        "    def load_chunks_from_json(self):\n",
        "        print(\"Loading chunks from JSON file...\")\n",
        "\n",
        "        with open(self.json_file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        chunks = data.get('chunks', [])\n",
        "        metadata = data.get('metadata', {})\n",
        "\n",
        "        print(f\"Loaded {len(chunks)} chunks\")\n",
        "        print(f\"Sources: {metadata.get('unique_sources', 'Unknown')}\")\n",
        "\n",
        "        return chunks, metadata\n",
        "\n",
        "    def setup_embeddings_and_vectorstore(self, chunks):\n",
        "        print(\"Setting up embeddings and vectorstore...\")\n",
        "\n",
        "        self.embedding_model = HuggingFaceEmbeddings(\n",
        "            model_name=\"all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "            encode_kwargs={'normalize_embeddings': True}\n",
        "        )\n",
        "\n",
        "        documents = []\n",
        "        for chunk in chunks:\n",
        "            doc = Document(\n",
        "                page_content=chunk['text'],\n",
        "                metadata={\n",
        "                    'source': chunk['metadata']['source'],\n",
        "                    'url': chunk['metadata']['url'],\n",
        "                    'chunk_id': chunk['id']\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "\n",
        "        self.vectorstore = Chroma.from_documents(\n",
        "            documents=documents,\n",
        "            embedding=self.embedding_model,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        print(f\"Vectorstore created with {len(documents)} documents\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def setup_llm(self):\n",
        "        print(\"Loading language model...\")\n",
        "\n",
        "        model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    torch_dtype=torch.bfloat16,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "                device_info = \"GPU with bfloat16\"\n",
        "            else:\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    torch_dtype=torch.float32,\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "                device_info = \"CPU with float32\"\n",
        "        except Exception as e:\n",
        "            print(f\"Primary loading failed, using fallback: {e}\")\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            device_info = \"fallback with float16\"\n",
        "\n",
        "        print(f\"Model loaded on {device_info}\")\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            return_full_text=False,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=True,\n",
        "            temperature=0.1,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "            template=\"\"\"You are an IBM watsonx documentation assistant. Provide accurate, concise answers based on the provided context.\n",
        "\n",
        "Guidelines:\n",
        "- Use only information from the provided context\n",
        "- Be direct and specific\n",
        "- Include relevant URLs when mentioned in the context\n",
        "- If information is not available, state this clearly\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        )\n",
        "\n",
        "        self.llm_chain = LLMChain(llm=llm, prompt=prompt_template, verbose=False)\n",
        "        print(\"Language model setup complete\")\n",
        "\n",
        "        return self.llm_chain\n",
        "\n",
        "    def retrieve_context(self, query: str, k: int = 4) -> str:\n",
        "        if not self.vectorstore:\n",
        "            return \"\"\n",
        "\n",
        "        docs = self.vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "        if not docs:\n",
        "            return \"\"\n",
        "\n",
        "        context_parts = []\n",
        "        seen_sources = set()\n",
        "\n",
        "        for doc in docs:\n",
        "            source = doc.metadata.get('source', 'Unknown Source')\n",
        "            url = doc.metadata.get('url', '')\n",
        "\n",
        "            if source not in seen_sources:\n",
        "                context_parts.append(f\"Source: {source}\")\n",
        "                if url:\n",
        "                    context_parts.append(f\"URL: {url}\")\n",
        "                seen_sources.add(source)\n",
        "                context_parts.append(\"\")\n",
        "\n",
        "            context_parts.append(doc.page_content.strip())\n",
        "            context_parts.append(\"\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def clean_response(self, response: str) -> str:\n",
        "        if isinstance(response, dict):\n",
        "            response = response.get(\"text\", str(response))\n",
        "\n",
        "        response = response.strip()\n",
        "\n",
        "        cleanup_phrases = [\n",
        "            \"Based on the context\", \"According to the context\",\n",
        "            \"The context mentions\", \"From the provided information\",\n",
        "            \"The provided context shows\", \"Based on the information provided\"\n",
        "        ]\n",
        "\n",
        "        for phrase in cleanup_phrases:\n",
        "            response = response.replace(phrase, \"\").strip()\n",
        "\n",
        "        response = \" \".join(response.split())\n",
        "\n",
        "        return response\n",
        "\n",
        "    def query(self, question: str) -> dict:\n",
        "        if not self.vectorstore or not self.llm_chain:\n",
        "            return {\n",
        "                \"error\": \"Pipeline not properly initialized\",\n",
        "                \"answer\": \"System not ready. Please initialize the pipeline first.\"\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            context = self.retrieve_context(question, k=4)\n",
        "\n",
        "            if not context:\n",
        "                return {\n",
        "                    \"question\": question,\n",
        "                    \"answer\": \"I don't have information about that in the IBM watsonx documentation.\",\n",
        "                    \"sources\": []\n",
        "                }\n",
        "\n",
        "            response = self.llm_chain.invoke({\"question\": question, \"context\": context})\n",
        "            answer = self.clean_response(response.get(\"text\", response))\n",
        "\n",
        "            docs = self.vectorstore.similarity_search(question, k=3)\n",
        "            sources = []\n",
        "            seen_titles = set()\n",
        "\n",
        "            for doc in docs:\n",
        "                title = doc.metadata.get('source', 'Unknown Source')\n",
        "                url = doc.metadata.get('url', '')\n",
        "\n",
        "                if title not in seen_titles:\n",
        "                    sources.append({\"title\": title, \"url\": url})\n",
        "                    seen_titles.add(title)\n",
        "\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"sources\": sources,\n",
        "                \"context_used\": True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"error\": f\"Error processing query: {str(e)}\",\n",
        "                \"answer\": \"I encountered an error while processing your question. Please try again.\"\n",
        "            }\n"
      ],
      "metadata": {
        "id": "HIUdsXztPTan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Helper Functions\n",
        "def upload_json_file():\n",
        "    \"\"\"Upload JSON file to Colab\"\"\"\n",
        "    print(\"Please upload your IBM_WatsonX_chunks JSON file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file uploaded!\")\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"Uploaded file: {filename}\")\n",
        "    return filename\n",
        "\n",
        "def initialize_pipeline():\n",
        "    \"\"\"Initialize all pipeline components\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"IBM watsonx RAG Pipeline Setup\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    json_filename = upload_json_file()\n",
        "    json_file_path = f\"/content/{json_filename}\"\n",
        "    print(f\"JSON file ready at: {json_file_path}\")\n",
        "\n",
        "    print(\"\\nInitializing RAG Pipeline...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    rag_pipeline = WatsonXRAGPipeline(json_file_path)\n",
        "    chunks, metadata = rag_pipeline.load_chunks_from_json()\n",
        "    vectorstore = rag_pipeline.setup_embeddings_and_vectorstore(chunks)\n",
        "    llm_chain = rag_pipeline.setup_llm()\n",
        "\n",
        "    print(\"\\nPipeline initialization complete!\")\n",
        "    print(\"Ready to answer questions about IBM watsonx.\")\n",
        "\n",
        "    return rag_pipeline\n",
        "\n",
        "def start_chat(rag_pipeline):\n",
        "    \"\"\"Interactive chat interface\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"IBM watsonx Documentation Assistant\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Ask questions about IBM watsonx documentation.\")\n",
        "    print(\"Commands: 'exit' to quit\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            if user_input.lower() in {'exit', 'quit', 'stop', 'q'}:\n",
        "                print(\"\\nGoodbye! Thank you for using the IBM watsonx Documentation Assistant.\")\n",
        "                break\n",
        "\n",
        "            result = rag_pipeline.query(user_input)\n",
        "\n",
        "            if 'error' in result and result['error']:\n",
        "                print(f\"Error: {result['answer']}\")\n",
        "            else:\n",
        "                print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "                if result.get('sources'):\n",
        "                    print(\"\\nRelevant documentation:\")\n",
        "                    for i, source in enumerate(result['sources'], 1):\n",
        "                        print(f\"{i}. {source['title']}\")\n",
        "                        if source['url']:\n",
        "                            print(f\"   {source['url']}\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nSession interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {str(e)}\")\n",
        "            print(\"Please try again.\")\n"
      ],
      "metadata": {
        "id": "DUpw90uOPTch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Main Execution Pipeline\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    try:\n",
        "        rag_pipeline = initialize_pipeline()\n",
        "        start_chat(rag_pipeline)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "        print(\"Please check your setup and try again.\")"
      ],
      "metadata": {
        "id": "D8GsAShbPrf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "tXkrGlMzPriJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}